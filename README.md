# 项目架构图

<img width="3508" height="2640" alt="稳赞系统架构图" src="https://github.com/user-attachments/assets/ea086ee2-eb87-4320-8a08-62b496e0e295" />
## 点赞数据时序图
<img width="1294" height="675" alt="PixPin_2025-09-18_17-49-31" src="https://github.com/user-attachments/assets/edd73504-4870-4a12-866f-6ce08fcecb59" />

# 技术选型

## RedisCluster和Redis读写集群技术选型

| 分类 | 功能项 | Redis Cluster | Redis 主从复制+读写分离 | 技术选型分析 |
|------|--------|---------------|------------------------|-------------|
| **架构设计** | **架构模式** | 原生分布式集群 | 主从复制 + 哨兵模式 | Redis Cluster为官方分布式方案，主从复制为传统高可用方案 |
|  | **数据分片** | 自动分片(16384 slots) | 无分片 | Cluster自动数据分片，主从复制全量数据复制 |
|  | **写扩展性** | 多主节点写入 | 单主节点写入 | Cluster支持多主写入，大幅提升写性能 |
|  | **读扩展性** | 多节点读取 | 多从节点读取 | 两者都支持读扩展，Cluster扩展性更强 |
| **数据一致性** | **数据同步** | 异步复制 | 异步/半同步复制 | 两者都是最终一致性，主从可配置半同步 |
|  | **数据分区** | 哈希槽分区 | 无分区 | Cluster通过哈希槽实现数据分布 |
|  | **跨节点事务** | 不支持 | 不支持 | 两者都不支持跨节点事务 |
| **高可用** | **故障转移** | 自动故障转移 | 哨兵自动切换 | 两者都支持自动故障转移 |
|  | **故障恢复时间** | 秒级 | 秒级 | 故障检测和切换时间相近 |
|  | **数据可靠性** | 多副本保障 | 多副本保障 | 两者都通过副本保证数据安全 |
| **性能** | **写入吞吐量** | 线性扩展 | 单主瓶颈 | Cluster写性能随节点数线性增长 |
|  | **读取吞吐量** | 线性扩展 | 线性扩展 | 两者读性能都可线性扩展 |
|  | **跨节点操作** | 需要重定向 | 无跨节点操作 | Cluster跨节点操作有额外开销 |
| **运维成本** | **部署复杂度** | 中等 | 简单 | Cluster部署和配置相对复杂 |
|  | **监控管理** | 需要集群管理工具 | 相对简单 | Cluster监控需要关注更多指标 |
|  | **数据迁移** | 自动数据平衡 | 手动数据同步 | Cluster支持自动数据迁移 |
|  | **客户端支持** | 需要集群感知客户端 | 普通客户端即可 | Cluster需要特殊客户端支持 |

**选型结论：** 在本项目中，因为T0级别为高并发高可用，Redis Cluster是支持分片的，在多个分片上可以并发的进行读写操作，可以极高的提升性能。而且Redis Cluster是去中心化的，相比于读写集群降低了单点故障带来的影响，所以选用Redis Cluster。

## Pulsar和Kafka技术选型

[Pulsar官方文档](https://pulsar.apache.org/docs/4.1.x/)

| 分类 | 功能项 | Pulsar | Kafka | 技术选型分析 |
|------|--------|--------|-------|-------------|
| **功能** | **消费推拉模式** | push | pull | Pulsar服务端推送提供更高实时性 |
|  | **延迟队列** | 支持 | 不支持 | Pulsar提供延迟消息，Kafka需自行实现 |
|  | **死信队列** | 支持 | 不支持 | Pulsar提供完善的重试和死信机制 |
|  | **优先级队列** | 不支持 | 不支持 | 两者均不支持消息优先级 |
|  | **消息回溯** | 支持 | 支持 | 两者都支持按偏移量重新消费 |
|  | **消息持久化** | 支持 | 支持 | 两者都提供持久化保证 |
|  | **消息确认机制** | Offset + 单条确认 | Offset 确认 | Pulsar支持更细粒度的消息确认 |
|  | **消息TTL** | 支持 | 不支持 | Pulsar支持消息消费超时后自动过期 |
|  | **多租户隔离** | 支持 | 不支持 | Pulsar原生支持多租户 |
|  | **消息顺序性** | 流模式有序 | 分区有序 | Pulsar流模式全局有序，Kafka仅分区有序 |
|  | **消息查询** | 支持 | 不支持 | Pulsar支持按属性查询消息 |
|  | **消息模式** | 流模式 + 队列模式 | 流模式 | Pulsar支持更丰富的消息模式 |
| **性能** | **消息可靠性** | Ack Quorum Size(Qa) | request.required.acks | 两者都提供可配置的可靠性级别 |
|  | **单机吞吐量** | 百万级QPS | 百万级QPS | 两者在吞吐量方面性能相当 |
|  | **消息延迟** | ms级 | ms级 | 两者延迟表现相近 |
|  | **支持主题数** | 上百万个 | 几十到几百 | Pulsar支持海量主题，Kafka在海量主题时性能严重受损 |
| **运维** | **高可用** | 分布式架构 | 分布式架构 | 两者都提供高可用保证 |
|  | **跨地域容灾** | 支持 | 不支持 | Pulsar内置多地域复制功能 |
|  | **集群扩容** | 增加节点，通过新增分片负载均衡 | 增加节点，通过复制数据均衡 | Pulsar扩容更平滑，无需数据重平衡 |

**选型结论：** 在本项目中，Pulsar对于高并发高可用需求支持能力更强，原生提供延迟队列和死信队列支撑更丰富的业务场景，计算和存储分离架构以及Broker无状态的服务机制提供更简单的水平扩容能力，支持百万级别的Topic，相比于Kafka可用性更强。并且Pulsar对于云原生提供很强的支持性，而云原生也是未来的发展趋势。因此本项目选择Pulsar作为MQ。

## TiDB和MySQL+ShardingJDBC技术选型

[TiDB官方文档](https://docs.pingcap.com/zh/tidb/stable/overview/?source=welcome)

| 分类 | 功能项 | TiDB | MySQL + ShardingJDBC | 技术选型分析 |
|------|--------|------|---------------------|-------------|
| **架构设计** | **架构模式** | 原生分布式NewSQL | 单机数据库 + 应用层分片 | TiDB原生分布式，ShardingJDBC为中间件方案会引入一层负载降低性能 |
|  | **数据分片** | 自动分片 | 手动分片配置 | TiDB自动管理分片，ShardingJDBC需预先规划 |
|  | **计算存储分离** | 支持 | 不支持 | TiDB可独立扩展计算和存储节点 |
|  | **SQL兼容性** | MySQL 5.7协议兼容 | 原生MySQL兼容 | 都高度兼容MySQL语法 |
| **扩展性** | **水平扩展** | 在线弹性扩展 | 支持但需停机 | TiDB扩容无感知，ShardingJDBC扩容复杂 |
|  | **扩展粒度** | 细粒度(计算存储分离架构) | 粗粒度(整个分片) | TiDB扩展更灵活 |
|  | **最大数据量** | PB级别 | TB级别 | TiDB适合海量数据场景 |
| **一致性** | **分布式事务** | 支持(乐观锁) | 支持(最终一致性) | TiDB提供强一致性事务 |
|  | **跨分片查询** | 透明支持 | 支持但性能有损 | TiDB自动优化分布式查询 |
|  | **全局索引** | 支持 | 不支持 | TiDB支持跨节点的全局索引 |
| **性能** | **读写性能** | 高并发读写 | 高并发读，写受分片限制 | TiDB写性能更优 |
|  | **复杂查询** | 优化器智能优化 | 需要手动优化分片查询 | TiDB对复杂查询支持更好 |
|  | **热点处理** | 自动热点Region调度 | 手动分片键调整 | TiDB自动负载均衡 |
| **高可用** | **故障恢复** | 自动故障转移 | 依赖主从复制 | TiDB Raft协议保证高可用 |
|  | **数据备份** | 分布式快照 | 传统备份方式 | TiDB备份更高效 |
|  | **多活部署** | 支持 | 有限支持 | TiDB原生支持多数据中心 |
| **运维成本** | **部署复杂度** | 中等 | 简单到复杂 | 简单场景MySQL更易部署 |
|  | **日常运维** | 自动化运维 | 手动分片管理 | TiDB运维更自动化 |
|  | **监控告警** | 完善监控体系 | 需要自行搭建 | TiDB监控更全面 |

**选型结论：** 在本项目高并发场景中，分布式NewSQL代替MySQL集群，可以实时在线扩容，支撑业务从零到亿级用户的完整生命周期。原生分布式设计，性能更优。多副本自动故障转移，业务不停机，可以实现服务高可用。自动热点调度，轻松应对突发流量。
